# 🗣️ How LLMs Assess Public Speaking? Methodology of Explaining LLM Judgments through Linguistic Patterns and Rhetorical Criteria.

**Evaluating GPT Annotations of Public Speaking Performances through Lexical and Criteria-Based Explainability**

## 🧠 Overview

This repository contains code, annotations, and analysis from our study investigating how GPT-4o-mini evaluates public speaking performances based solely on textual transcripts. We compare model outputs to expert annotations across well-defined rhetorical and linguistic criteria, as well as abstract subjective dimensions like *Persuasiveness* and *Clarity of Language*.

## 🔍 What We Did

* Introduced a **new annotated dataset** based on *Ma Thèse en 180 secondes*, with expert and GPT-4o-mini annotations.
* Conducted a **multi-layered comparison** between expert and LLM annotations using both subjective dimensions and structural criteria.
* Evaluated alignment with **audience and jury ratings**, revealing differences in human and model preferences.
* Linked annotations to **interpretable lexical features** (e.g., repetition, MTLD, passive voice) to investigate explainability.
* Grounded evaluation strategies in **public speaking theory** and persuasion models like ELM and HSM.

## 📦 Contents

* `data/` – Annotated transcripts with expert and GPT-4o-mini scores
* `scripts/` – Code for annotation, prompting, and feature extraction
* `analysis/` – Correlation results and visualizations
* `paper/` – LaTeX source files for the article

## 📄 Citation

If you use this dataset or code, please cite our paper (coming soon).

