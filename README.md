# ğŸ—£ï¸ How LLMs Assess Public Speaking? Methodology of Explaining LLM Judgments through Linguistic Patterns and Rhetorical Criteria.

**Evaluating GPT Annotations of Public Speaking Performances through Lexical and Criteria-Based Explainability**

## ğŸ§  Overview

This repository contains code, annotations, and analysis from our study investigating how GPT-4o-mini evaluates public speaking performances based solely on textual transcripts. We compare model outputs to expert annotations across well-defined rhetorical and linguistic criteria, as well as abstract subjective dimensions like *Persuasiveness* and *Clarity of Language*.

## ğŸ” What We Did

* Introduced a **new annotated dataset** based on *Ma ThÃ¨se en 180 secondes*, with expert and GPT-4o-mini annotations.
* Conducted a **multi-layered comparison** between expert and LLM annotations using both subjective dimensions and structural criteria.
* Evaluated alignment with **audience and jury ratings**, revealing differences in human and model preferences.
* Linked annotations to **interpretable lexical features** (e.g., repetition, MTLD, passive voice) to investigate explainability.
* Grounded evaluation strategies in **public speaking theory** and persuasion models like ELM and HSM.

## ğŸ“¦ Contents

* `data/` â€“ Annotated transcripts with expert and GPT-4o-mini scores
* `scripts/` â€“ Code for annotation, prompting, and feature extraction
* `analysis/` â€“ Correlation results and visualizations
* `paper/` â€“ LaTeX source files for the article

## ğŸ“„ Citation

If you use this dataset or code, please cite our paper (coming soon).

